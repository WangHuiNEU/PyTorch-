# 1. nn.Module()

> Author: 王辉
>
> Time: 2021/12/30

[TOC]

![微信图片_20211223085830](1. nn.Module().assets/微信图片_20211223085830.jpg)

# PyTorch 

`PyTorch` 是一种动态构建计算图的深度学习框架。计算图是用来描述运算过程的有向无环图，节点表示数据，边表示操作或者运算，

<img src="1. nn.Module().assets/image-20211230094623146.png" alt="image-20211230094623146" style="zoom: 33%;" />

由用户手动创建的Tensor称为叶子节点，默认不计算梯度；由Function函数计算得到的Tensor是非叶子节点，requires_grad由参与计算的tensor决定。在整个计算图中，只要有一个节点`requires_grad = True` ，所有依赖该节点的节点的`requires_grad`均为True. 

```python
# 参考代码如下：
import torch
w = torch.tensor([1.], requires_grad=True)
x = torch.tensor([2.], requires_grad=True)
# y=(x+w)*(w+1)
a = torch.add(w, x)     # retain_grad()
b = torch.add(w, 1)
y = torch.mul(a, b)
# y 求导
y.backward() # ！求导
# 打印 w 的梯度，就是 y 对 w 的导数
print(w.grad)
```

PyTorch中的torch.Tensor有8个属性，

```python
# torch.Tensor的属性
data
dtype
shape
device
requires_grad
grad
grad_fn
is_leaf
```

## 1. 区别`torch.Tensor()`和`torch.tensor()`

`torch.Tensor()`是`torch.FloatTensor()`的别名，其`torch.dtype`是一个float32类型的张量，不能够指定数据类型，但是可以转化为一个已知数据类型的张量，使用`type_as(tensor)`将张量转化为给定数据的数据类型。因此，`torch.Tensor()`就是一个单精度浮点类型的张量，定义的默认的`requires_grad`都为False.

`torch.tensor()`是一个函数，定义为`torch.tensor(data, dtype=None, device=None, requires_grad=False)` 。可以根据相应的数据生成不同的数据类型。

```python
a = torch.tensor([1,2,3])
>>> a.type()
'torch.LongTensor'
a = torch.tensor([1.,2.,3.])
>>> a.type()
'torch.FloatTensor'
```

这里讲明这里面的区别的原因是因为，**在模型训练的时候lable必须是`torch.LongTensor`类型**，其他类型模型会报错，因此，label尽量不要使用`torch.Tensor()`来定义，推荐使用`torch.LongTensor()`或者`torch.tensor([],dtype="torch.LongTensor")` 来进行定义。

## 2. requires_grad, grad_fn和grad的区别

```python
"requires_grad": 如果需要为张量计算梯度，则为True，否则为False。我们使用pytorch创建tensor时，可以指定requires_grad为True(默认为False).
*******************************************************************************
"grad_fn": grad_fn用来记录变量是怎么来的，方便计算梯度，y = x*3,grad_fn记录了y由x计算的过程。创建该张量时所用的方法 (函数)。而在反向传播求导梯度时需要用到该属性。
*******************************************************************************
"grad": 当执行完了backward()之后，通过x.grad查看x的梯度值。
```

```python
>>x = torch.ones(2, 2, requires_grad=True)
>>print(x.grad_fn)  # None
>>y = x + 2
>>print(y.grad_fn)  # <AddBackward object at 0x1100477b8>
```

**由于x是直接创建的，所以它没有grad_fn，而y是通过一个假发操作创建的，所以y有grad_fn，**像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None。

==注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。==

## 3. 叶子节点和非叶子节点的进一步理解

> 参考：https://zhuanlan.zhihu.com/p/85506092

在pytorch中，神经网络层中的权值w的tensor均为叶子节点；自己定义的tensor例如a=torch.tensor([1.0])定义的节点是叶子节点；一个有趣的现象是：

```python
>a=torch.tensor([1.0])
>a.is_leaf
True
>b=a+1
>b.is_leaf
True
```

可以看出b竟然也是叶节点！这件事可以这样理解，单纯从数值关系上b=a+1，b确实依赖a。但是从pytorch的看来，一切是为了反向求导，a的requires_grad属性为False，其不要求获得梯度，那么a这个tensor在反向传播时其实是“无意义”的，可认为是游离在计算图之外的，故b仍然为叶子节点，如下图

<img src="1. nn.Module().assets/v2-34ec12743bb3dec8b7a7f1e288d8f827_720w.jpg" alt="img" style="zoom:50%;" />

再例如下图的计算图，本来是叶子节点是可以正常进行反向传播计算梯度的：

![img](1. nn.Module().assets/v2-8691b38f5d21071139ec2347cb9d8e68_720w.jpg)

但是使用detach()函数将某一个非叶子节点剥离成为叶子节点后

![img](1. nn.Module().assets/v2-7876805d6ce3d15cc0e039630f62e9fb_720w.jpg)

无论requires_grad属性为何值，原先的叶子节点求导通路中断，便无法获得梯度数值了。

==![[公式]](https://www.zhihu.com/equation?tex=x) 和 ![[公式]](https://www.zhihu.com/equation?tex=w) 是叶子节点，其他所有节点都依赖于叶子节点。叶子节点的概念主要是为了节省内存，在计算图中的一轮反向传播结束之后，非叶子节点的梯度是会被释放的。==

```python
# 查看叶子结点
print("is_leaf:\n", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)
# 查看梯度
print("gradient:\n", w.grad, x.grad, a.grad, b.grad, y.grad)
```

```python
# 输出
is_leaf:
 True True False False False
gradient:
 tensor([5.]) tensor([2.]) None None None
```

非叶子节点的梯度为空，如果在反向传播结束之后仍然需要保留非叶子节点的梯度，可以对节点使用`retain_grad()`方法。

## 4. 静态图和动态图的区别以及理解

PyTorch 采用的是动态图机制 (Dynamic Computational Graph)，而 Tensorflow 采用的是静态图机制 (Static Computational Graph)。

动态图是运算和搭建同时进行，也就是可以先计算前面的节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。PyTorch 里的很多写法跟其他 Python 库的代码的使用方法是完全一致的，没有任何额外的学习成本。

静态图是先搭建图，然后再输入数据进行运算。优点是高效，因为静态计算是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。但是不灵活。TensorFlow 每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用 Python 的 while 循环语句，需要使用辅助函数 tf.while_loop 写成 TensorFlow 内部的形式。

---

---

---

# torch.nn.Module()函数

`torch.nn.Module`是所有神经网络模块的基类，所有的神经网络模型都应该继承这个基类，即PyTorch都应该是`nn.mudule`的子类，并进行重载`init` (初始化)和 `forward`(前向传播)函数。即在`init`进行模型子模块的构建，在`forward`进行子模块的拼接。每个类都有一个对应的`nn.funcational`函数，类定义了所需要的`arguments`和模块的`parameters` ,在forward函数中将`arguments`和`parameters`传给`nn.functional`的对应的函数来实现forward功能

---

# 1. ______init______函数 

初始化函数

```python
    def __init__(self):
        """
        Initializes internal Module state, shared by both nn.Module and ScriptModule.
        """
        torch._C._log_api_usage_once("python.nn_module")

        self.training = True
        self._parameters = OrderedDict()                      # 保存当前module的训练参数                
        self._buffers = OrderedDict()                         # 保存当前moduile的非训练参数
        self._non_persistent_buffers_set = set()              # 
        self._backward_hooks = OrderedDict()                  # Backward 完成后会被调用的 hook
        self._is_full_backward_hook = None                    
        self._forward_hooks = OrderedDict()                   # Forward 完成后会被调用的 hook
        self._forward_pre_hooks = OrderedDict()               # Forward 完成前会被调用的 hook
        self._state_dict_hooks = OrderedDict()                # 得到 state_dict 以后会被调用的 hook
        self._load_state_dict_pre_hooks = OrderedDict()       # load state_dict 前会被调用的 hook
        self._modules = OrderedDict()                         # 子神经网络模块
```

- `__init__`函数首先会调用`torch.__C.__log_api_usage_once("python.nn_module")`. 表示添加`python.nn_module`这一个新的API触发点，用于监测这个API的使用情况。[官方文档](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/notes/large_scale_deployments.html%23api-usage-logging)。

- `self.training = True` 标志网络的状态，主要影响在模型训练和测试的时候是否开启或者关闭`bn`和`dropout`
- ` self._parameters = OrderedDict()` 保存当前module的训练参数
- `self._buffers` 保存当前moduile的非训练参数，如bn的running_mean和running_var
- `self._modules` 保存当前module中的子module（不是自定义模型中的所有module）
- 其余属性都是用户自定义的hook函数

## hook技术简绍

> https://blog.csdn.net/qq_28753373/article/details/104129877

利用hook，我们可以在不改变网络输入输出的结构，方便地获取、改变网络中间节点的值和梯度。hook分为针对tensor的hook和针对module的hook。先介绍针对tensor的hook，用法为tensor.register_hook(grad_fn)，grad_fn为自定义函数，用于对tensor的梯度做相关处理，输入为tensor的梯度，输出为一个tensor(改变回传的梯度值)或None(不改变回传的梯度值)。也可在一个节点处添加多个hook，按照定义的顺序依次执行。register_hook返回一个RemovableHandle类的对象，执行该类的remove方法可将计算图中相应的hook移除。

```python
h1 = y_.register_hook(lambda grad: print(grad))
h2 = y_.register_hook(lambda grad: 2*grad)
z.backward()
print(y.grad, y_.grad)
# tensor([2.], dtype=torch.float64)
# tensor([5.], dtype=torch.float64) tensor([6.], dtype=torch.float64)

h1.remove()  # hook函数应在使用后及时移除,以免增加计算量
h2.remove()
```

y_节点的梯度计算完成后，依次执行两个hook函数。第一个函数打印该节点的梯度值，结果为2；第二个函数将该节点的梯度值*2后再反向传播，即该节点传给y节点的梯度值为4，则y节点的梯度值由1变为2，在累加到之前的结果上，故y.grad的值变为5。需要注意的是，hook函数改变的是节点传出的梯度值大小，其梯度本身并未改变，仍为2，故累加到之前的结果上，y_.grad的值为6。

由上述分析可知，针对tensor的hook的本质是，在节点梯度计算完成后且向后传播前，都调用一次grad_fn函数，在grad_fn中对梯度进行处理，将处理后的梯度向后传播。

```python
import torch

x = torch.tensor([2], dtype=float, requires_grad=True)
y = torch.tensor([6], dtype=float, requires_grad=True)
y_ = y / 2
z = x ** 2 + y_ * 2

z.backward(retain_graph=True)
print(x.grad, y.grad, y_.grad, z.grad)
# tensor([4.], dtype=torch.float64) tensor([1.], dtype=torch.float64) None None

print(torch.autograd.grad(z, [x, y, y_], retain_graph=True))
# (tensor([4.], dtype=torch.float64), tensor([1.], dtype=torch.float64), tensor([2.], dtype=torch.float64))

y_.retain_grad() # 保留指定的非叶节点的梯度;必须定义在backward()之前
z.backward(retain_graph=True)
print(x.grad, y.grad, y_.grad, z.grad)
# tensor([8.], dtype=torch.float64) tensor([2.], dtype=torch.float64) tensor([2.], dtype=torch.float64) None
# 此时梯度被累积

x.grad.zero_()
z.backward(retain_graph=True)
print(x.grad, y.grad, y_.grad, z.grad)
# tensor([4.], dtype=torch.float64) tensor([3.], dtype=torch.float64) tensor([4.], dtype=torch.float64) None
# x的梯度事先被清零,计算正确;其他节点的梯度继续累积

h1 = y_.register_hook(lambda grad: print(grad))
h2 = y_.register_hook(lambda grad: 2*grad)
z.backward()
print(y.grad, y_.grad)
# tensor([2.], dtype=torch.float64)
# tensor([5.], dtype=torch.float64) tensor([6.], dtype=torch.float64)

h1.remove()  # hook函数应在使用后及时移除,以免增加计算量
h2.remove()
```

观察上述代码的输出结果，对比三种方法：

torch.autograd.grad和hook技术都是需要在进行一次反向传播来计算需要的梯度值，能够得到准确的梯度值。而retain_grad是保留之后所有反向传播过程中该节点的梯度缓存，若要得到准确的梯度值，则要先清空缓存。除此之外，hook技术能够改变节点回传的梯度值，作用更加广泛，推荐使用。

然而，在一个较大的网络中，我们很难获得中间变量显式的变量名，进而对其执行register_hook操作。此时要获得这些中间变量的梯度就要使用针对module的register_forward_hook和register_backward_hook来分别获得前向和反向传播时，中间变量输入和输出的 feature/gradient。
register_forward_hook(hook_fn)可获取module的feature前向传播时的输入和输出，其中hook_fn的函数签名为:

```python
hook_fn(module, input, output) -> Tensor or None
```

register_backward_hook(hook_fn)可获取module的gradient反向传播时的输入和输出(注意此时的输入输出的方向是前向传播的方向)，其中hook_fn的函数签名为：

```python
hook_fn(module, grad_input, grad_output) -> Tensor or None
```

使用实例如下：

```python
import torch
import torch.nn as nn

class net(nn.Module):
    def __init__(self):
        super(net, self).__init__()

        self.fc1 = nn.Linear(1, 2, bias=False)
        self.fc2 = nn.Linear(2, 1, bias=False)
        self._initialize()

    def _initialize(self):
        self.fc1.weight = nn.Parameter(
            torch.tensor([[2.], 
                          [2.]]))
        self.fc2.weight = nn.Parameter(torch.tensor([2., 2.]))

    def forward(self, x):
        return self.fc2(self.fc1(x))


def forward_hook_fn(module, input, output):
    print(module)
    print('feature input:', input)
    print('feature output:', output)


def backward_hook_fn(module, grad_input, grad_output):
    print(module)
    print('gradient input:', grad_input)
    print('gradient output:', grad_output)


model = net()
modules = model.children()
for m in modules:
    m.register_forward_hook(forward_hook_fn)    # 作用于输出计算完成后,向前传播前
    m.register_backward_hook(backward_hook_fn)  # 作用与梯度计算完成后,向后传播前

x = torch.tensor([2.], requires_grad=True)
out = model(x)
out.backward()
print(x.grad)
# Linear(in_features=1, out_features=2, bias=False)
# feature input: (tensor([2.], requires_grad=True),)
# feature output: tensor([4., 4.], grad_fn=<SqueezeBackward3>)
# Linear(in_features=2, out_features=1, bias=False)
# feature input: (tensor([4., 4.], grad_fn=<SqueezeBackward3>),)
# feature output: tensor(16., grad_fn=<DotBackward>)
# Linear(in_features=2, out_features=1, bias=False)
# gradient input: (tensor([2., 2.]), tensor([4., 4.]))
# gradient output: (tensor(1.),)
# Linear(in_features=1, out_features=2, bias=False)
# gradient input: (tensor([[2., 2.]]),)
# gradient output: (tensor([2., 2.]),)
# tensor([8.])
```

当这两个函数均作用与包含子module的复合module时，结果如下：

```python
model.register_forward_hook(forward_hook_fn)
model.register_backward_hook(backward_hook_fn)
# net(
#   (fc1): Linear(in_features=1, out_features=2, bias=False)
#   (fc2): Linear(in_features=2, out_features=1, bias=False)
# )
# feature input: (tensor([2.], requires_grad=True),)
# feature output: tensor(16., grad_fn=<DotBackward>)
# net(
#   (fc1): Linear(in_features=1, out_features=2, bias=False)
#   (fc2): Linear(in_features=2, out_features=1, bias=False)
# )
# gradient input: (tensor([2., 2.]), tensor([4., 4.]))
# gradient output: (tensor(1.),)
```

根据输出结果可知，register_forward_hook作用于复合module，只能得到最顶层复合module的输入和输出信息；register_backward_hook作用于复合module时，只能得到最后一个module的梯度信息。
另外，还有一个针对module的hook函数为register_forward_pre_hook(hook_fn)，它的hook_fn作用于module对输入做前向运算之前，因此可改变module的输入。hook_fn的函数签名为：

```python
hook_fn(module, input) -> Tensor or None
```

register_forward_pre_hook和register_forward_hook的区别是：前者作用于前向运算之前，可修改module的输入；后者作用于前向运算之后，可修改module向后传播的值。

---



# 2. 添加新元素

```python
def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:
向self._buffers注册新元素

def register_parameter(self, name: str, param: Optional[Parameter]) -> None:   
向self._parameters注册新元素


def add_module(self, name: str, module: Optional['Module']) -> None:
向self._modules注册新元素
```



# 3. 类型转换

```python
def cuda(self: T, device: Optional[Union[int, device]] = None) -> T
将所有的parameters和buffers移动到gpu

def cpu(self: T) -> T:
将所有的parameters和buffers移动到cpu

def type(self: T, dst_type: Union[dtype, str]) -> T
将所有的parameters和buffers都转换为指定的目标类型

def float(self: T) -> T:
将所有的parameters和buffers都转换为float类型

def double(self: T) -> T
将所有的parameters和buffers都转换为double类型

def half(self: T) -> T:
将所有的parameters和buffers都转换为float16类型

#***************************************
# to 函数有四种用法
This can be called as

        .. function:: to(device=None, dtype=None, non_blocking=False)

        .. function:: to(dtype, non_blocking=False)

        .. function:: to(tensor, non_blocking=False)

        .. function:: to(memory_format=torch.channels_last)
                
device (:class:`torch.device`): the desired device of the parameters and buffers in this module
    
dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module
    
tensor (torch.Tensor): Tensor whose dtype and device are the desire dtype and device for all parameters and buffers in this module
    
memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument)
    
# 下面详细简绍
@overload
def to(self: T, device: Optional[Union[int, device]] = ..., dtype: Optional[Union[dtype, str]])
转移到指定的device


@overload
def to(self: T, dtype: Union[dtype, str], non_blocking: bool = ...)
转换为指定的dtype


@overload
def to(self: T, tensor: Tensor, non_blocking: bool = ...) 
将tensor属性(dtype和device)转换到与指定tensor相同

基类
def to(self, *args, **kwargs)
           
```

==上述所有函数的功能均借助_apply完成==

```python
def _apply(self, fn):
        for module in self.children():
            module._apply(fn)

        def compute_should_use_set_data(tensor, tensor_applied):
            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
                # If the new tensor has compatible tensor type as the existing tensor,
                # the current behavior is to change the tensor in-place using `.data =`,
                # and the future behavior is to overwrite the existing tensor. However,
                # changing the current behavior is a BC-breaking change, and we want it
                # to happen in future releases. So for now we introduce the
                # `torch.__future__.get_overwrite_module_params_on_conversion()`
                # global flag to let the user control whether they want the future
                # behavior of overwriting the existing tensor or not.
                return not torch.__future__.get_overwrite_module_params_on_conversion()
            else:
                return False

        for key, param in self._parameters.items():
            if param is not None:
                # Tensors stored in modules are graph leaves, and we don't want to
                # track autograd history of `param_applied`, so we have to use
                # `with torch.no_grad():`
                with torch.no_grad():
                    param_applied = fn(param)
                should_use_set_data = compute_should_use_set_data(param, param_applied)
                if should_use_set_data:
                    param.data = param_applied
                else:
                    assert isinstance(param, Parameter)
                    assert param.is_leaf
                    self._parameters[key] = Parameter(param_applied, param.requires_grad)

                if param.grad is not None:
                    with torch.no_grad():
                        grad_applied = fn(param.grad)
                    should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)
                    if should_use_set_data:
                        param.grad.data = grad_applied
                    else:
                        assert param.grad.is_leaf
                        self._parameters[key].grad = grad_applied.requires_grad_(param.grad.requires_grad)

        for key, buf in self._buffers.items():
            if buf is not None:
                self._buffers[key] = fn(buf)

        return self
```

_apply函数遍历了所有的parameters（如果有grad还要遍历grad）和buffers，对它们应用fn，并将fn作用的结果注册到相应的容器中。

- 通过 self.children() 进行递归的调用
- 对 self._parameters 中的参数及其 gradient 通过 function 进行处理
- 对 self._buffers 中的 buffer 逐个通过 function 来进行处理

---

nn.Module 还实现了一个 apply 函数，与 _apply() 函数不同的是，apply 函数只是简单地递归调用了 self.children() 去处理自己以及子模块，如下面的代码所示。

```python
def apply(self: T, fn: Callable[['Module'], None]) -> T:
    for module in self.children():
        module.apply(fn)
    fn(self)
    return self
```

apply 函数和 _apply 函数的区别在于，_apply() 是**专门针对 parameter 和 buffer** 而实现的一个“仅供内部使用”的接口，但是 apply 函数是“公有”接口 （Python 对类的“公有”和“私有”区别并不是很严格，一般通过单前导下划线来区分）。apply 实际上可以通过修改 fn 来实现 _apply 能实现的功能，同时还可以实现其他功能，如下面给出的重新初始化参数的例子。

> 参考：https://zhuanlan.zhihu.com/p/340453841

- Example: 参数重新初始化

可以自定义一个 init_weights 函数，通过 `net.apply(init_weights)` 来初始化模型权重。

```python
@torch.no_grad()
def init_weights(m):
    print(m)
    if type(m) == nn.Linear:
        m.weight.fill_(1.0)
        print(m.weight)

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
net.apply(init_weights)
```

---

# 4. hook注册

```python
def register_backward_hook(
        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
    )

def register_full_backward_hook(
        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
    )
# 向self._backward_hooks注册新元素


def register_forward_pre_hook(self, hook: Callable[..., None])
# 向self._forward_pre_hooks注册新元素


def register_forward_hook(self, hook: Callable[..., None]) 
# 向self._forward_hooks注册新元素
```

这四个hook注册函数，用于注册被应用于全局的 hook。这 3 个函数会将 hook 分别注册进 3 个全局的 OrderedDict，使得所有的 nn.Module 的子类实例在运行的时候都会触发这些 hook。

```python
self.register_backward_hook--------------->self._backward_hooks
self.register_forward_pre_hook------------>self._forward_pre_hooks
self.register_forward_hook---------------->self._forward_hooks
```

# 5. 模型保存和加载

```python
@overload
def state_dict(self, destination: T_destination, prefix: str = ..., keep_vars: bool = ...)
# 返回一个包含模型所有parameters和buffers的字典。需要注意的是，所有的参数在被添加到字典中之前，都要经过self._state_dict_hooks中的hook函数处理。该函数是个递归函数，每次递归调用了辅助函数_save_to_state_dict来将当前module的子module参数添加到字典中


def load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]',strict: bool = True)
# 将指定的state_dict中的参数加载到当前的module中。需要注意的是，所有的参数在被添加到当前模型的_parameters和 _ buffers之前，都要经过self. _load_state_dict_pre_hooks中hook函数处理。

# 该函数中定义了一个递归函数load。load每次递归时调用辅助函数_load_from_state_dict来添加当前module的子module参数。

# 该函数有一个输入参数strict，默认为True。strict为True时，会返回一个namedtuple。该tuple有两个属性：missing_keys和unexpected_keys。missing_keys是存储当前module有而待加载的state_dict中没有的参数的列表；unexpected_keys是存储当前module没有而待加载的state_dict中有的参数的列表。
```



# 6. 信息查询

```python
def named_modules(self, memo: Optional[Set['Module']] = None, prefix: str = '')
# 返回模型中的所有module（包括模型本身）及其名称。返回值中有两个值，第一个为module的名（string格式），第二个为相应的module。返回顺序是自顶向下。
def modules(self) -> Iterator['Module']
# 返回模型中的所有module，调用named_modules完成.
```

```python
def named_children(self) -> Iterator[Tuple[str, 'Module']]
# 返回当前模型_modules中的所有元素及其名称。注意该函数与named_modules的区别：named_modules是递归函数，每次递归均查询当前module的 _modules，进而能够遍历模型中的所有module；而named_chilldren只查询整个模型的 _modules
def children(self) -> Iterator['Module']
# 返回当前模型_modules中的所有元素，调用named_modules来完成。
```

```python
def named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Parameter]]
# 返回模型中所有的可训练参数及其名称。
def parameters(self, recurse: bool = True) -> Iterator[Parameter]
# 返回模型中所有的可训练参数
```

```python
def named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Tensor]]
# 返回模型中所有的非训练参数及其名称
def buffers(self, recurse: bool = True) -> Iterator[Tensor]
# 返回模型中所有的非训练参数
```



# 7. 状态设置

```python
def train(self: T, mode: bool = True)
def eval(self: T) -> T  
def requires_grad_(self: T, requires_grad: bool = True) -> T
def zero_grad(self, set_to_none: bool = False) -> None
```

- train：将模型设为训练模式；主要影响BN、Dropout等module。
- eval：将模型设为评估模式；主要影响BN、Dropout等module。调用train来实现。
- require_grad_：设置模型中所有的Parameter的require_grad属性，即是否要计算梯度。调用parameters()来实现。
- zero_grad：将模型中所有的Parameter的梯度置为零，并将其从计算图中分离。调用parameters()来实现
  

---

**流程：nn.Module 在被调用的时候，一般是以 module(input) 的形式，此时会首先调用 `self.__call__`，接下来这些 hooks 在模块被调用时候的执行顺序如下图所示：**

![img](1. nn.Module().assets/v2-5ecf55ee46df3cec3c908acd1dbaf37d_720w.png)

> 图参考：https://zhuanlan.zhihu.com/p/340453841

_call_impl 的代码实现如下。注意到 _call_impl 在定义以后被直接赋值给了 `__call__` 。同时我们注意到在 torch._C._get_tracing_state() 为 True 的时候，nn.Module 会通过 _slow_forward() 来调用 forward 函数而非直接调用 forward 函数，这一功能主要用于 **JIT**。





![屏幕截图 2021-12-30 155913](1. nn.Module().assets/屏幕截图 2021-12-30 155913.png)



![屏幕截图 2021-12-30 155936](1. nn.Module().assets/屏幕截图 2021-12-30 155936.png)



![屏幕截图 2021-12-30 155950](1. nn.Module().assets/屏幕截图 2021-12-30 155950.png)



![屏幕截图 2021-12-30 160009](1. nn.Module().assets/屏幕截图 2021-12-30 160009.png)

![屏幕截图 2021-12-30 160023](1. nn.Module().assets/屏幕截图 2021-12-30 160023.png)

---

参考文章：

> https://blog.csdn.net/goes_on/article/details/109097234
>
> https://blog.csdn.net/qq_28753373/article/details/104268663
>
> https://blog.csdn.net/qq_28753373/article/details/104129877
>
> https://github.com/zhangxiann/PyTorch_Practice/blob/master/README.md
>
> https://baijiahao.baidu.com/s?id=1676325181081727690&wfr=spider&for=pc
>
> https://zhuanlan.zhihu.com/p/88712978
>
> https://zhuanlan.zhihu.com/p/191648279
>
> https://zhuanlan.zhihu.com/p/340453841 !
>
> https://pytorch.org/docs/1.7.1/generated/torch.nn.Module.html#torch.nn.Module
>
> https://zhuanlan.zhihu.com/p/203405689